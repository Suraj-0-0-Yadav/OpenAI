{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e1ee38",
   "metadata": {},
   "source": [
    "# [How_to_count_tokens_with_tiktoken.ipynb](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5ceea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "openai.api_key = dotenv_values('../.env')['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47370165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ceffa8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding1 = tiktoken.get_encoding(\"p50k_base\")\n",
    "encoding2 = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94a4bba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 1609, 5963, 374, 2294, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.encode(\"tiktoken is great!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5790fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding.encode(\"tiktoken is great!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2496d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text = \"\"\"\n",
    "All children, except one, grow up. They soon know that they will grow\n",
    "up, and the way Wendy knew was this. One day when she was two years old\n",
    "she was playing in a garden, and she plucked another flower and ran\n",
    "with it to her mother. I suppose she must have looked rather\n",
    "delightful, for Mrs. Darling put her hand to her heart and cried, “Oh,\n",
    "why can’t you remain like this for ever!” This was all that passed\n",
    "between them on the subject, but henceforth Wendy knew that she must\n",
    "grow up. You always know after you are two. Two is the beginning of the\n",
    "end.\n",
    "\n",
    "Of course they lived at 14, and until Wendy came her mother was the\n",
    "chief one. She was a lovely lady, with a romantic mind and such a sweet\n",
    "mocking mouth. Her romantic mind was like the tiny boxes, one within\n",
    "the other, that come from the puzzling East, however many you discover\n",
    "there is always one more; and her sweet mocking mouth had one kiss on\n",
    "it that Wendy could never get, though there it was, perfectly\n",
    "conspicuous in the right-hand corner.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eee87891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding1.encode(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a7bbb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding2.encode(book_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35ef21",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f39ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding1 = tiktoken.encoding_for_model('text-davinci-003')\n",
    "encoding2 = tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0dffc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding1.encode(book_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "433c3512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoding2.encode(book_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c34f26",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "152ebad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_model(text:str, model_name:str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(model_name=model_name)\n",
    "    num_token = len(encoding.encode(text))\n",
    "    \n",
    "    return num_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acd77a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_model(book_text, 'text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fe910c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_model(book_text, 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9268b22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_model(book_text, 'gpt-3.5-turbo-16k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03e206c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_model(book_text, 'gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "815961d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_model(book_text, 'gpt-4-32k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce4d33e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241aa1c",
   "metadata": {},
   "source": [
    "# Count tokens for ChatCompletion API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4afbe75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "#         print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "#         print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c9a85f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9b3bc27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_messages(example_messages, 'gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa1ae0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5f2d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9275b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "| Model Name         |   Num Tokens |\n",
      "+====================+==============+\n",
      "| \u001b[1;31m\u001b[1mgpt-3.5-turbo-0301\u001b[0m |          127 |\n",
      "+--------------------+--------------+\n",
      "| \u001b[1;31m\u001b[1mgpt-3.5-turbo-0613\u001b[0m |          129 |\n",
      "+--------------------+--------------+\n",
      "| \u001b[1;31m\u001b[1mgpt-3.5-turbo\u001b[0m      |          129 |\n",
      "+--------------------+--------------+\n",
      "| \u001b[1;31m\u001b[1mgpt-4-0314\u001b[0m         |          129 |\n",
      "+--------------------+--------------+\n",
      "| \u001b[1;31m\u001b[1mgpt-4-0613\u001b[0m         |          129 |\n",
      "+--------------------+--------------+\n",
      "| \u001b[1;31m\u001b[1mgpt-4\u001b[0m              |          129 |\n",
      "+--------------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "headers = [\"Model Name\", \"Num Tokens\"]\n",
    "\n",
    "color_code = \"\\033[1;31m\"\n",
    "bold_code = \"\\033[1m\"\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"gpt-4-0314\",\n",
    "    \"gpt-4-0613\",\n",
    "    \"gpt-4\",]:\n",
    "    \n",
    "    num_tokens = num_tokens_from_messages(example_messages, model)\n",
    "    colored_and_bold_model_name = f\"{color_code}{bold_code}{model}\\033[0m\"\n",
    "    data.append([colored_and_bold_model_name, num_tokens])\n",
    "\n",
    "table = tabulate(data, headers, tablefmt=\"grid\")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8b8d9a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------------------------------+---------------------------+\n",
      "| Model Name         |   Num Tokens (num_tokens_from_messages()) |   Num Tokens (OpenAI API) |\n",
      "+====================+===========================================+===========================+\n",
      "| gpt-3.5-turbo-0301 |                                       127 |                       127 |\n",
      "+--------------------+-------------------------------------------+---------------------------+\n",
      "| gpt-3.5-turbo-0613 |                                       129 |                       129 |\n",
      "+--------------------+-------------------------------------------+---------------------------+\n",
      "| gpt-3.5-turbo      |                                       129 |                       127 |\n",
      "+--------------------+-------------------------------------------+---------------------------+\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "headers = [\"Model Name\", \"Num Tokens (num_tokens_from_messages())\", \"Num Tokens (OpenAI API)\"]\n",
    "\n",
    "for model in [\n",
    "    \"gpt-3.5-turbo-0301\",\n",
    "    \"gpt-3.5-turbo-0613\",\n",
    "    \"gpt-3.5-turbo\",\n",
    "]:\n",
    "    num_tokens = num_tokens_from_messages(example_messages, model)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=example_messages,\n",
    "        temperature=0,\n",
    "        max_tokens=1,\n",
    "    )\n",
    "    api_num_tokens = response[\"usage\"][\"prompt_tokens\"]\n",
    "    data.append([model, num_tokens, api_num_tokens])\n",
    "\n",
    "table = tabulate(data, headers, tablefmt=\"grid\")\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559409d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
